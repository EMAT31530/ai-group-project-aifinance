{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "own_env.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHecT8knyJeh"
      },
      "source": [
        "# !pip install tensorflow\n",
        "# !pip install gym\n",
        "# !pip install keras\n",
        "# !pip install keras-rl2\n",
        "# !pip install yfinance\n",
        "# !pip install ta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvGVwBvnyJio"
      },
      "source": [
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box\n",
        "import numpy as np\n",
        "import random\n",
        "import yfinance as yf\n",
        "import datetime\n",
        "import ta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFrx6K53yJkv"
      },
      "source": [
        "class Trading(Env):\n",
        "  def __init__(self, df, lookback_win = 1, initial_balance = 1000):\n",
        "    self.df = df.dropna().reset_index()\n",
        "    self.df = self.df.drop(['index'], axis=1)\n",
        "    self.lookback_win = lookback_win\n",
        "    self.current_step = self.lookback_win  # Initial point\n",
        "    self.state = self.df.iloc[self.current_step - self.lookback_win: self.current_step]  # State is a selection of 10 points\n",
        "    self.end_step = len(self.df) - self.lookback_win\n",
        "    self.initial_balance = initial_balance\n",
        "    self.balance = self.initial_balance\n",
        "    self.stock_held = 0\n",
        "    self.net_worth = self.balance\n",
        "    self.prev_net_worth = self.balance\n",
        "    self.buys = 0\n",
        "    self.sells = 0\n",
        "    self.holds = 0\n",
        "    self.dupe = 0\n",
        "\n",
        "    self.action_space = Discrete(3)  # Actions we can take 0 = hold, 1 = buy, 2 = sell\n",
        "    self.state_size = (self.lookback_win, len(df.columns))\n",
        "\n",
        "  def reset(self):\n",
        "    self.current_step = self.lookback_win  # Initial point\n",
        "    self.state = self.df.iloc[self.current_step - self.lookback_win: self.current_step]  # State is a selection of 10 points\n",
        "    self.end_step = len(self.df) - self.lookback_win\n",
        "    self.balance = self.initial_balance\n",
        "    self.stock_held = 0\n",
        "    self.net_worth = self.balance\n",
        "    self.prev_net_worth = self.balance\n",
        "    self.buys = 0\n",
        "    self.sells = 0\n",
        "    self.holds = 0\n",
        "    self.dupe = 0\n",
        "\n",
        "    return self.state\n",
        "\n",
        "  def step(self, action):\n",
        "    self.current_step += 1\n",
        "    self.state = self.df.iloc[self.current_step - self.lookback_win: self.current_step]\n",
        "    current_price = self.df.loc[self.current_step, 'Close']\n",
        "\n",
        "    self.prev_net_worth = self.net_worth\n",
        "    self.net_worth = self.stock_held * current_price + self.balance\n",
        "\n",
        "    # Actions\n",
        "    if action == 0:\n",
        "      self.holds += 1\n",
        "      # calculating reward\n",
        "      reward = self.net_worth - self.prev_net_worth\n",
        "    \n",
        "    elif action == 1 and self.balance > 0:  # BUY\n",
        "      self.buys += 1\n",
        "      self.stock_bought = self.balance/current_price\n",
        "      self.balance = 0\n",
        "      self.stock_held += self.stock_bought\n",
        "\n",
        "      # calculating reward\n",
        "      reward = self.prev_net_worth - self.net_worth\n",
        "      info = 'buy'\n",
        "\n",
        "    elif action == 2 and self.stock_held > 0:  # SELL\n",
        "      self.sells += 1\n",
        "      self.balance += self.stock_held * current_price\n",
        "      self.stock_held = 0\n",
        "\n",
        "      # calculating reward\n",
        "      reward = self.net_worth - self.prev_net_worth\n",
        "      info = 'sell'\n",
        "\n",
        "    else:\n",
        "      self.dupe += 1\n",
        "      reward = -10\n",
        "\n",
        "\n",
        "    if self.current_step >= self.end_step:  # Check if shower is done\n",
        "      done = True\n",
        "    else:\n",
        "      done = False\n",
        "\n",
        "    info = []  # Set info placeholder\n",
        "\n",
        "    return self.state, reward, done, info  # Return step information\n",
        "\n",
        "  def render(self):\n",
        "    pass\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRKoKrHSyJnK"
      },
      "source": [
        "# Returns ticker_DF (Ticker is another name for stock)\n",
        "def yfinance_data(ticker_symbol, start_date, period):  # returns data frame of prices for given ticker\n",
        "    ticker_data = yf.Ticker(ticker_symbol)\n",
        "    today = datetime.datetime.today().isoformat()\n",
        "    ticker_DF = ticker_data.history(perod=period, start=start_date, end=today[:10])\n",
        "    return ticker_DF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVOmd6RoyJpJ"
      },
      "source": [
        "df = yfinance_data('NKE', '2010-1-1', '1d')\n",
        "df = df.drop(['Dividends', 'Stock Splits', 'Open', 'Volume', 'High', 'Low'], axis=1)\n",
        "df = df.reset_index(drop=True, inplace=False)\n",
        "\n",
        "df['Macd'] = ta.trend.macd_diff(df['Close'], window_slow=26, window_fast=12, window_sign=9, fillna=False)\n",
        "df['Rsi'] = ta.momentum.rsi(df['Close'], window=14, fillna=False)\n",
        "\n",
        "window = 1\n",
        "for i in range(window):\n",
        "  for column in df:\n",
        "    df[column + ' Lag: ' + str(i+1)] = df[column].shift(i+1)\n",
        "df = df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5045OCuR1tyM"
      },
      "source": [
        "env = Trading(df)\n",
        "\n",
        "state_shape = env.state_size\n",
        "actions = env.action_space.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HXh3boOwRwX"
      },
      "source": [
        "### Making my own agent ###\n",
        "\n",
        "segements = [15]\n",
        "discrete_os_size = segements * state_shape[1]  # 15x7\n",
        "discrete_win_size_array = []\n",
        "column_low_val_array = []\n",
        "for column in df:\n",
        "  high = df[column].max()\n",
        "  low = df[column].min()\n",
        "  column_low_val_array.append(low)\n",
        "  discrete_win_size_array.append((high - low) / segements[0]+1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hAeXk_CwR22",
        "outputId": "1abbc998-3d9a-40e1-889a-1ed75697dbe3"
      },
      "source": [
        "### Creating Q table ###\n",
        "q_table = np.random.uniform(low=-100, high=100, size=(discrete_os_size + [actions]))\n",
        "print(np.shape(q_table))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15, 15, 15, 15, 15, 15, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuC2lIYBwR53",
        "outputId": "a7b0b661-2c25-482c-95a7-75716bcc8fa0"
      },
      "source": [
        "learning_rate = 0.1\n",
        "discount = 0.95\n",
        "episodes = 500\n",
        "SHOW_EVERY = 50\n",
        "epsilon = 0.5\n",
        "start_epsilon_decaying = 1\n",
        "end_epsilon_decaying = episodes // 2\n",
        "\n",
        "epsilon_decay_value = epsilon/(end_epsilon_decaying - start_epsilon_decaying)\n",
        "\n",
        "\n",
        "# (10x7 state - 10x7 low value) / 10x7 win size\n",
        "def get_discrete_state(state):\n",
        "  state = state.to_numpy()\n",
        "  discrete_state = (state - column_low_val_array) / discrete_win_size_array\n",
        "  discrete_state = discrete_state.astype(np.int)\n",
        "  return tuple(map(tuple, discrete_state))\n",
        "\n",
        "discrete_state = get_discrete_state(env.reset())\n",
        "print(discrete_state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((0, 1, 5, 0, 1, 6),)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvajtt8ywSAd",
        "outputId": "64a5b835-6e2e-4330-e7e4-1bafa29cd5ad"
      },
      "source": [
        "for episode in range(1, episodes + 1):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  discrete_state = get_discrete_state(state)\n",
        "\n",
        "  while not done:\n",
        "\n",
        "    if np.random.random() > epsilon:\n",
        "      action = np.argmax(q_table[discrete_state[0]])\n",
        "    else:\n",
        "      action = np.random.randint(3)\n",
        "    n_state, reward, done, info = env.step(action)\n",
        "    new_discrete_state = get_discrete_state(n_state)\n",
        "\n",
        "    if not done:\n",
        "      max_future_q = np.max(q_table[new_discrete_state[0]])\n",
        "      current_q = q_table[discrete_state[0] + (action, )]\n",
        "      new_q = (1 - learning_rate) * current_q + learning_rate * (reward + discount * max_future_q)\n",
        "      q_table[discrete_state[0] + (action, )] = new_q\n",
        "\n",
        "    discrete_state = new_discrete_state\n",
        "  if end_epsilon_decaying >= episode >= start_epsilon_decaying:\n",
        "    epsilon -= epsilon_decay_value\n",
        "\n",
        "  if episode % SHOW_EVERY == 0:\n",
        "    print('Episode:{} Net_worth:{} Buys:{} Sells:{} Holds:{} Duplicates:{}'.format(episode, env.net_worth, env.buys, env.sells, env.holds, env.dupe))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode:50 Net_worth:3029.778891992126 Buys:320 Sells:319 Holds:1445 Duplicates:698\n",
            "Episode:100 Net_worth:6441.5580617595115 Buys:286 Sells:285 Holds:1628 Duplicates:583\n",
            "Episode:150 Net_worth:7545.65336331791 Buys:279 Sells:278 Holds:1531 Duplicates:694\n",
            "Episode:200 Net_worth:13767.991805821004 Buys:234 Sells:233 Holds:1697 Duplicates:618\n",
            "Episode:250 Net_worth:20019.936851996434 Buys:222 Sells:221 Holds:1811 Duplicates:528\n",
            "Episode:300 Net_worth:20019.936851996434 Buys:222 Sells:221 Holds:1811 Duplicates:528\n",
            "Episode:350 Net_worth:20019.936851996434 Buys:222 Sells:221 Holds:1811 Duplicates:528\n",
            "Episode:400 Net_worth:20019.936851996434 Buys:222 Sells:221 Holds:1811 Duplicates:528\n",
            "Episode:450 Net_worth:20019.936851996434 Buys:222 Sells:221 Holds:1811 Duplicates:528\n",
            "Episode:500 Net_worth:20019.936851996434 Buys:222 Sells:221 Holds:1811 Duplicates:528\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}